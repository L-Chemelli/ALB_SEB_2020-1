{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5kAL6BmwqDlfVpWWjXO6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L-Chemelli/ALB_SEB_2020-1/blob/master/Teachable_hotmart_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação do ambiente"
      ],
      "metadata": {
        "id": "0hHzBxXJu_Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FzWbN5z39M3i",
        "outputId": "635c278d-3ba2-4038-fbd2-fc0f89fe81e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=80112ec5d1bebf658b7faf58e99b492be7f2e0af37336dda6c9ad3e9bee3c64b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType, TimestampType\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "W_IBqA3aBbLm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"hotmart_teachable_ae_test\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZiUILABiEQk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pergunta 1"
      ],
      "metadata": {
        "id": "stgC23-HJ-sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação do código"
      ],
      "metadata": {
        "id": "RaFWJwwHuzwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição do schema\n",
        "schema = StructType([\n",
        "    StructField(\"purchase_id\", IntegerType(), True),\n",
        "    StructField(\"buyer_id\", IntegerType(), True),\n",
        "    StructField(\"prod_item_id\", IntegerType(), True),\n",
        "    StructField(\"order_date\", DateType(), True),\n",
        "    StructField(\"release_date\", DateType(), True),\n",
        "    StructField(\"producer_id\", IntegerType(), True),\n",
        "    StructField(\"purchase_partition\", IntegerType(), True),\n",
        "    StructField(\"prod_item_partition\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Dados\n",
        "data = [\n",
        "    (55, 15947, 5, date(2022, 12, 1), date(2022, 12, 1), 852852, 5, 5),\n",
        "    (56, 369798, 746520, date(2022, 12, 25), date(2022, 12, 25), 963963, 6, 0),\n",
        "    (57, 147, 98736, date(2021, 7, 3), date(2021, 7, 3), 963963, 7, 6),\n",
        "    (58, 986533, 6565, date(2021, 10, 12), None, 200478, 8, 5)\n",
        "]\n",
        "\n",
        "# Criação do DataFrame\n",
        "purchase = spark.createDataFrame(data, schema)"
      ],
      "metadata": {
        "id": "WHMBCJGg9qIF"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição do schema\n",
        "schema = StructType([\n",
        "    StructField(\"prod_item_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"item_quantity\", IntegerType(), True),\n",
        "    StructField(\"purchase_value\", DoubleType(), True),\n",
        "    StructField(\"prod_item_partition\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Dados para o DataFrame\n",
        "data = [\n",
        "    (1, 69, 5, 500.00, 5),\n",
        "    (5, 69, 120, 1.00, 0),\n",
        "    (98736, 37, 69, 25.00, 6),\n",
        "    (3, 96, 369, 140.00, 5)\n",
        "]\n",
        "\n",
        "# Criação do DataFrame\n",
        "product_item  = spark.createDataFrame(data, schema)"
      ],
      "metadata": {
        "id": "YWxhRiqZBCSa"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purchase.createOrReplaceTempView('purchase')\n",
        "product_item.createOrReplaceTempView('product_item')"
      ],
      "metadata": {
        "id": "og6C0II6_2db"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Quais são os 50 maiores produtores em faturamento ($) de 2021?"
      ],
      "metadata": {
        "id": "jMt4Wt-cGdjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado_1 = spark.sql('''\n",
        "select distinct producer_id,\n",
        "                sum(purchase_value*item_quantity) faturamento\n",
        "from purchase pch\n",
        "  inner join product_item pdi on pch.prod_item_id = pdi.prod_item_id\n",
        "where 1=1\n",
        "  and pch.release_date is not null\n",
        "  and YEAR(pch.release_date) = 2021\n",
        "group by producer_id\n",
        "order by faturamento desc\n",
        "limit 50\n",
        "''')\n",
        "\n",
        "resultado_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eV1PtgIZCLnJ",
        "outputId": "f7a7d789-eac2-41f8-d71a-25e97271bd50"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|producer_id|faturamento|\n",
            "+-----------+-----------+\n",
            "|     963963|     1725.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Quais são os 2 produtos que mais faturaram ($) de cada produtor?\n"
      ],
      "metadata": {
        "id": "0ed7k2mNGfTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado_2 = spark.sql('''\n",
        "with faturamento_produto_produtor as (\n",
        "select distinct producer_id,\n",
        "                product_id,\n",
        "                sum(purchase_value*item_quantity) faturamento\n",
        "from purchase pch\n",
        "  inner join product_item pdi on pch.prod_item_id = pdi.prod_item_id\n",
        "where 1=1\n",
        "  and pch.release_date is not null\n",
        "  and YEAR(pch.release_date) = 2021\n",
        "group by producer_id, product_id\n",
        "),\n",
        "rank_faturamento as (\n",
        "select producer_id,\n",
        "       product_id,\n",
        "       faturamento,\n",
        "       row_number() over (partition by producer_id order by faturamento desc) posicao_faturamento\n",
        "from faturamento_produto_produtor\n",
        ")\n",
        "select producer_id,\n",
        "       product_id,\n",
        "       faturamento\n",
        "from rank_faturamento\n",
        "where 1=1\n",
        "  and posicao_faturamento <= 2\n",
        "order by producer_id, faturamento desc;\n",
        "''')\n",
        "\n",
        "resultado_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wcfy74r8Gj37",
        "outputId": "b95c9d1d-11fd-4e0e-e2e8-1bda053c5004"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+\n",
            "|producer_id|product_id|faturamento|\n",
            "+-----------+----------+-----------+\n",
            "|     963963|        37|     1725.0|\n",
            "+-----------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pergunta 2"
      ],
      "metadata": {
        "id": "RSVOhb78LQG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação para o código"
      ],
      "metadata": {
        "id": "UWRLj9t4uuTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DateType, DecimalType\n",
        "from datetime import datetime\n",
        "\n",
        "# Função para converter strings em datetime\n",
        "def to_datetime(date_str):\n",
        "    return datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def to_date(date_str):\n",
        "    return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "# Esquema para o primeiro DataFrame\n",
        "schema1 = StructType([\n",
        "    StructField(\"transaction_datetime\", TimestampType(), True),\n",
        "    StructField(\"transaction_date\", DateType(), True),\n",
        "    StructField(\"purchase_id\", IntegerType(), True),\n",
        "    StructField(\"buyer_id\", IntegerType(), True),\n",
        "    StructField(\"prod_item_id\", IntegerType(), True),\n",
        "    StructField(\"order_date\", DateType(), True),\n",
        "    StructField(\"release_date\", DateType(), True),\n",
        "    StructField(\"producer_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Dados para o primeiro DataFrame convertidos\n",
        "purchase = [\n",
        "    (to_datetime(\"2023-01-20 22:00:00\"), to_date(\"2023-01-20\"), 55, 15947, 5, to_date(\"2023-01-20\"), to_date(\"2023-01-20\"), 852852),\n",
        "    (to_datetime(\"2023-01-26 00:01:00\"), to_date(\"2023-01-26\"), 56, 369798, 746520, to_date(\"2023-01-25\"), None, 963963),\n",
        "    (to_datetime(\"2023-02-05 10:00:00\"), to_date(\"2023-02-05\"), 55, 160001, 5, to_date(\"2023-01-20\"), to_date(\"2023-01-20\"), 852852),\n",
        "    (to_datetime(\"2023-02-26 03:00:00\"), to_date(\"2023-02-26\"), 69, 160001, 18, to_date(\"2023-02-26\"), to_date(\"2023-02-28\"), 96967),\n",
        "    (to_datetime(\"2023-07-15 09:00:00\"), to_date(\"2023-07-15\"), 55, 160001, 5, to_date(\"2023-01-20\"), to_date(\"2023-03-01\"), 852852)\n",
        "]\n",
        "\n",
        "# Criação do DataFrame\n",
        "df_purchase = spark.createDataFrame(purchase, schema=schema1)\n",
        "\n",
        "# Esquema para o segundo DataFrame\n",
        "schema2 = StructType([\n",
        "    StructField(\"transaction_datetime\", TimestampType(), True),\n",
        "    StructField(\"transaction_date\", DateType(), True),\n",
        "    StructField(\"purchase_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"item_quantity\", IntegerType(), True),\n",
        "    StructField(\"purchase_value\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Dados para o segundo DataFrame convertidos\n",
        "product_item = [\n",
        "    (to_datetime(\"2023-01-20 22:02:00\"), to_date(\"2023-01-20\"), 55, 696969, 10, 50.00),\n",
        "    (to_datetime(\"2023-01-25 23:59:59\"), to_date(\"2023-01-25\"), 56, 808080, 120, 2400.00),\n",
        "    (to_datetime(\"2023-02-26 03:00:00\"), to_date(\"2023-02-26\"), 69, 373737, 2, 2000.00),\n",
        "    (to_datetime(\"2023-07-12 09:00:00\"), to_date(\"2023-07-12\"), 55, 696969, 10, 55.00)\n",
        "]\n",
        "\n",
        "# Criação do DataFrame\n",
        "df_product_item = spark.createDataFrame(product_item, schema=schema2)\n",
        "\n",
        "# Esquema para o terceiro DataFrame\n",
        "schema3 = StructType([\n",
        "    StructField(\"transaction_datetime\", TimestampType(), True),\n",
        "    StructField(\"transaction_date\", DateType(), True),\n",
        "    StructField(\"purchase_id\", IntegerType(), True),\n",
        "    StructField(\"subsidiary\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Dados para o terceiro DataFrame convertidos\n",
        "purchase_extra_info = [\n",
        "    (to_datetime(\"2023-01-23 00:05:00\"), to_date(\"2023-01-23\"), 55, \"nacional\"),\n",
        "    (to_datetime(\"2023-01-25 23:59:59\"), to_date(\"2023-01-25\"), 56, \"internacional\"),\n",
        "    (to_datetime(\"2023-02-28 01:10:00\"), to_date(\"2023-02-28\"), 69, \"nacional\"),\n",
        "    (to_datetime(\"2023-03-12 07:00:00\"), to_date(\"2023-03-12\"), 69, \"internacional\")\n",
        "]\n",
        "\n",
        "# Criação do DataFrame\n",
        "df_purchase_extra_info = spark.createDataFrame(purchase_extra_info, schema=schema3)\n",
        "\n",
        "# Exibindo os DataFrames\n",
        "df_purchase.show(truncate=False)\n",
        "df_product_item.show(truncate=False)\n",
        "df_purchase_extra_info.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TIf40uSlLOp4",
        "outputId": "6ec331c3-af69-4460-d17c-2e72c441d915"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+\n",
            "|transaction_datetime|transaction_date|purchase_id|buyer_id|prod_item_id|order_date|release_date|producer_id|\n",
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+\n",
            "|2023-01-20 22:00:00 |2023-01-20      |55         |15947   |5           |2023-01-20|2023-01-20  |852852     |\n",
            "|2023-01-26 00:01:00 |2023-01-26      |56         |369798  |746520      |2023-01-25|NULL        |963963     |\n",
            "|2023-02-05 10:00:00 |2023-02-05      |55         |160001  |5           |2023-01-20|2023-01-20  |852852     |\n",
            "|2023-02-26 03:00:00 |2023-02-26      |69         |160001  |18          |2023-02-26|2023-02-28  |96967      |\n",
            "|2023-07-15 09:00:00 |2023-07-15      |55         |160001  |5           |2023-01-20|2023-03-01  |852852     |\n",
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+\n",
            "\n",
            "+--------------------+----------------+-----------+----------+-------------+--------------+\n",
            "|transaction_datetime|transaction_date|purchase_id|product_id|item_quantity|purchase_value|\n",
            "+--------------------+----------------+-----------+----------+-------------+--------------+\n",
            "|2023-01-20 22:02:00 |2023-01-20      |55         |696969    |10           |50.0          |\n",
            "|2023-01-25 23:59:59 |2023-01-25      |56         |808080    |120          |2400.0        |\n",
            "|2023-02-26 03:00:00 |2023-02-26      |69         |373737    |2            |2000.0        |\n",
            "|2023-07-12 09:00:00 |2023-07-12      |55         |696969    |10           |55.0          |\n",
            "+--------------------+----------------+-----------+----------+-------------+--------------+\n",
            "\n",
            "+--------------------+----------------+-----------+-------------+\n",
            "|transaction_datetime|transaction_date|purchase_id|subsidiary   |\n",
            "+--------------------+----------------+-----------+-------------+\n",
            "|2023-01-23 00:05:00 |2023-01-23      |55         |nacional     |\n",
            "|2023-01-25 23:59:59 |2023-01-25      |56         |internacional|\n",
            "|2023-02-28 01:10:00 |2023-02-28      |69         |nacional     |\n",
            "|2023-03-12 07:00:00 |2023-03-12      |69         |internacional|\n",
            "+--------------------+----------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETL"
      ],
      "metadata": {
        "id": "UyPkFLEQupOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#Exemplo de leitura (Extract)\n",
        "# df_purchase = spark.read.table('purchase')\n",
        "# df_product_item = spark.read.table('product_item')\n",
        "# df_purchase_extra_info = spark.read.table('purchase_extra_info')\n",
        "\n",
        "#Janela que será utilizada\n",
        "window_spec = Window.partitionBy(\"purchase_id\", \"transaction_date\").orderBy(F.col(\"transaction_datetime\").desc())\n",
        "\n",
        "#Rank de todas as tabelas\n",
        "df_purchase = df_purchase.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "df_product_item = df_product_item.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "df_purchase_extra_info = df_purchase_extra_info.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "#Última atualização do dia\n",
        "df_purchase = df_purchase.filter(F.col(\"rank\") == 1).drop(\"rank\", \"transaction_datetime\")\n",
        "df_product_item = df_product_item.filter(F.col(\"rank\") == 1).drop(\"rank\", \"transaction_datetime\")\n",
        "df_purchase_extra_info = df_purchase_extra_info.filter(F.col(\"rank\") == 1).drop(\"rank\", \"transaction_datetime\")\n",
        "\n",
        "#Todo range de datas para cada purchase\n",
        "df_purchase_dates = df_purchase.select(F.col('transaction_date'),\n",
        "                                       F.col('purchase_id'))\n",
        "df_product_item_dates = df_product_item.select(F.col('transaction_date'),\n",
        "                                               F.col('purchase_id'))\n",
        "df_purchase_extra_info_dates = df_purchase_extra_info.select(F.col('transaction_date'),\n",
        "                                                              F.col('purchase_id'))\n",
        "\n",
        "df_combined = df_purchase_dates.union(df_product_item_dates)\n",
        "df_purchase_all_dates = df_combined.union(df_purchase_extra_info_dates)\\\n",
        "                         .select('*')\\\n",
        "                         .distinct()\n",
        "\n",
        "#Join Condition\n",
        "\n",
        "join_condition = [F.col('a.transaction_date') == F.col('b.transaction_date'), F.col('a.purchase_id') == F.col('b.purchase_id')]\n",
        "\n",
        "#Preenche as tabelas nas datas faltantes\n",
        "df_purchase = df_purchase_all_dates.alias('a').join(df_purchase.alias('b'), join_condition,'left').drop(F.col('b.purchase_id'), F.col('b.transaction_date'))\n",
        "df_product_item = df_purchase_all_dates.alias('a').join(df_product_item.alias('b'), join_condition,'left').drop(F.col('b.purchase_id'), F.col('b.transaction_date'))\n",
        "df_purchase_extra_info = df_purchase_all_dates.alias('a').join(df_purchase_extra_info.alias('b'), join_condition,'left').drop(F.col('b.purchase_id'), F.col('b.transaction_date'))\n",
        "\n",
        "#Ordenando DFs\n",
        "df_purchase = df_purchase.orderBy(\"purchase_id\", \"transaction_date\")\n",
        "df_product_item = df_product_item.orderBy(\"purchase_id\", \"transaction_date\")\n",
        "df_purchase_extra_info = df_purchase_extra_info.orderBy(\"purchase_id\", \"transaction_date\")\n",
        "\n",
        "# Janela para preencher vazios\n",
        "window_spec = Window.partitionBy(\"purchase_id\").orderBy(\"transaction_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Preencher os valores nulos\n",
        "def fill_nulls(df):\n",
        "    # Obter a lista das colunas (exceto transaction_date e purchase_id)\n",
        "    columns_to_fill = [c for c in df.columns if c not in ('transaction_date', 'purchase_id')]\n",
        "\n",
        "    # Definir a janela para preencher valores nulos, particionando por purchase_id\n",
        "    window_spec = Window.partitionBy(\"purchase_id\").orderBy(\"transaction_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "    # Aplicar o preenchimento a todas as colunas\n",
        "    df_filled = df\n",
        "    for column in columns_to_fill:\n",
        "        df_filled = df_filled.withColumn(\n",
        "            column,\n",
        "            F.last(column, ignorenulls=True).over(window_spec)\n",
        "        )\n",
        "\n",
        "    return df_filled\n",
        "\n",
        "df_purchase = fill_nulls(df_purchase)\n",
        "df_product_item = fill_nulls(df_product_item)\n",
        "df_purchase_extra_info = fill_nulls(df_purchase_extra_info)\n",
        "\n",
        "# Mostrar o DataFrame preenchido\n",
        "df_output = df_purchase.alias('a').join(df_product_item.alias('b'), join_condition, 'left').drop(F.col('b.purchase_id'), F.col('b.transaction_date'))\n",
        "df_output = df_output.alias('a').join(df_purchase_extra_info.alias('b'), join_condition, 'left').drop(F.col('b.purchase_id'), F.col('b.transaction_date'))\n",
        "\n",
        "#Versão mais recente do registro\n",
        "df_purchase_most_recent = df_purchase_all_dates.groupBy(\"purchase_id\") \\\n",
        "    .agg(F.max(\"transaction_date\").alias(\"latest_transaction_date\"))\n",
        "\n",
        "join_condition = [F.col('a.purchase_id') == F.col('b.purchase_id'), F.col('a.transaction_date') == F.col('b.latest_transaction_date')]\n",
        "\n",
        "df_output = df_output.alias('a').join(df_purchase_most_recent.alias('b'),join_condition, 'left')\\\n",
        "                                                    .withColumn('is_most_recent_transaction', F.when(F.col('latest_transaction_date').isNull(), False)\n",
        "                                                                                              .otherwise(True))\\\n",
        "                                                    .drop(F.col('b.purchase_id'), F.col('b.latest_transaction_date'))\\\n",
        "                                                    .withColumn(\"unique_id\", F.sha2(F.concat(F.col(\"transaction_date\"), F.lit(\"_\"), F.col(\"purchase_id\")), 256))\\\n",
        "                                                    .select(\n",
        "                                                            \"unique_id\",\n",
        "                                                            \"transaction_date\",\n",
        "                                                            \"purchase_id\",\n",
        "                                                            \"buyer_id\",\n",
        "                                                            \"prod_item_id\",\n",
        "                                                            \"order_date\",\n",
        "                                                            \"release_date\",\n",
        "                                                            \"producer_id\",\n",
        "                                                            \"product_id\",\n",
        "                                                            \"item_quantity\",\n",
        "                                                            \"purchase_value\",\n",
        "                                                            \"subsidiary\",\n",
        "                                                            \"is_most_recent_transaction\"\n",
        "                                                        )\n",
        "\n",
        "\n",
        "df_output.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "usCyY1jGNA-5",
        "outputId": "3204264e-0ebf-40b4-b1aa-6e611141187b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+----------+-------------+--------------+-------------+--------------------------+\n",
            "|           unique_id|transaction_date|purchase_id|buyer_id|prod_item_id|order_date|release_date|producer_id|product_id|item_quantity|purchase_value|   subsidiary|is_most_recent_transaction|\n",
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+----------+-------------+--------------+-------------+--------------------------+\n",
            "|41acafb0a9249af43...|      2023-01-20|         55|   15947|           5|2023-01-20|  2023-01-20|     852852|    696969|           10|          50.0|         NULL|                     false|\n",
            "|a2a61420bbe783a30...|      2023-01-23|         55|   15947|           5|2023-01-20|  2023-01-20|     852852|    696969|           10|          50.0|     nacional|                     false|\n",
            "|0e5dbfcb3a4ed1a48...|      2023-02-05|         55|  160001|           5|2023-01-20|  2023-01-20|     852852|    696969|           10|          50.0|     nacional|                     false|\n",
            "|ab00b8d9435f81fe2...|      2023-07-12|         55|  160001|           5|2023-01-20|  2023-01-20|     852852|    696969|           10|          55.0|     nacional|                     false|\n",
            "|ce0fe6a334505bd68...|      2023-07-15|         55|  160001|           5|2023-01-20|  2023-03-01|     852852|    696969|           10|          55.0|     nacional|                      true|\n",
            "|327d08086076e5625...|      2023-01-25|         56|    NULL|        NULL|      NULL|        NULL|       NULL|    808080|          120|        2400.0|internacional|                     false|\n",
            "|d69585f456d005103...|      2023-01-26|         56|  369798|      746520|2023-01-25|        NULL|     963963|    808080|          120|        2400.0|internacional|                      true|\n",
            "|0b55c0b02615bb4a0...|      2023-02-26|         69|  160001|          18|2023-02-26|  2023-02-28|      96967|    373737|            2|        2000.0|         NULL|                     false|\n",
            "|a4e8705ea381ec1fb...|      2023-02-28|         69|  160001|          18|2023-02-26|  2023-02-28|      96967|    373737|            2|        2000.0|     nacional|                     false|\n",
            "|263038f727135acf4...|      2023-03-12|         69|  160001|          18|2023-02-26|  2023-02-28|      96967|    373737|            2|        2000.0|internacional|                      true|\n",
            "+--------------------+----------------+-----------+--------+------------+----------+------------+-----------+----------+-------------+--------------+-------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create As Table"
      ],
      "metadata": {
        "id": "SUmjUgspulM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Se utilizado o PySpark, poderia ser simplesmente utilizado o Save as Table ao fim com partição, para criar a tabela com Schema final do código\n",
        "# Para o caso do problema pedido, o Create As Table poderia ser:\n",
        "spark.sql('''\n",
        " CREATE TABLE purchases_historical (\n",
        "    unique_id STRING,\n",
        "    transaction_date DATE,\n",
        "    purchase_id INT,\n",
        "    buyer_id INT,\n",
        "    prod_item_id INT,\n",
        "    order_date DATE,\n",
        "    release_date DATE,\n",
        "    producer_id INT,\n",
        "    product_id INT,\n",
        "    item_quantity INT,\n",
        "    purchase_value DECIMAL(10, 2),\n",
        "    subsidiary STRING,\n",
        "    is_most_recent_transaction BOOLEAN\n",
        ")\n",
        "PARTITIONED BY (transaction_date DATE)\n",
        "\n",
        "CREATE UNIQUE INDEX idx_unique_purchase_id ON TABLE purchases_historical (unique_id)\n",
        "CREATE INDEX idx_purchase_id ON TABLE purchases_historical (purchase_id)\n",
        "CREATE INDEX idx_transaction_date ON TABLE purchases_historical (transaction_date)\n",
        "CREATE INDEX idx_composite_purchase_date ON TABLE purchases_historical (purchase_id, transaction_date);\n",
        "''')"
      ],
      "metadata": {
        "id": "wlXxgRazfYZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_output.createOrReplaceTempView('purchases_historical')"
      ],
      "metadata": {
        "id": "MoQDgOqngMVU"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMV Diário Atual"
      ],
      "metadata": {
        "id": "6rkQIvb1mpHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = spark.sql('''\n",
        "select release_date,\n",
        "       subsidiary,\n",
        "       sum(purchase_value*item_quantity) faturamento\n",
        "from purchases_historical pch\n",
        "where 1=1\n",
        "  and pch.release_date is not null\n",
        "  and is_most_recent_transaction = TRUE\n",
        "group by release_date, subsidiary\n",
        "order by release_date desc\n",
        "''')\n",
        "\n",
        "resultado.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YisU7V-ImmSn",
        "outputId": "5c3690f7-b357-4d22-eaba-8f35f408c83d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+-----------+\n",
            "|release_date|   subsidiary|faturamento|\n",
            "+------------+-------------+-----------+\n",
            "|  2023-03-01|     nacional|      550.0|\n",
            "|  2023-02-28|internacional|     4000.0|\n",
            "+------------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMV Diário se posicionando no tempo"
      ],
      "metadata": {
        "id": "mFgz7gkKueJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = spark.sql('''\n",
        "with rank_registros as (\n",
        "select\n",
        "    unique_id,\n",
        "    ROW_NUMBER() OVER (PARTITION BY purchase_id ORDER BY transaction_date DESC) AS posicao_registro\n",
        "from purchases_historical\n",
        "where 1=1\n",
        " and transaction_date <= \"2023-02-27\"\n",
        "),\n",
        "posicionamento as (\n",
        "select *\n",
        "from purchases_historical pch\n",
        "  inner join rank_registros rr on pch.unique_id = rr.unique_id and rr.posicao_registro = 1\n",
        ")\n",
        "select release_date,\n",
        "       subsidiary,\n",
        "       sum(purchase_value*item_quantity) faturamento\n",
        "from posicionamento pch\n",
        "where 1=1\n",
        "  and pch.release_date is not null\n",
        "group by release_date, subsidiary\n",
        "order by release_date desc\n",
        "''')\n",
        "\n",
        "resultado.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "usPeOOVEgV4Z",
        "outputId": "2a55dbc3-ed73-4a74-ceba-8da9cf8bc8f9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+-----------+\n",
            "|release_date|subsidiary|faturamento|\n",
            "+------------+----------+-----------+\n",
            "|  2023-02-28|      NULL|     4000.0|\n",
            "|  2023-01-20|  nacional|      500.0|\n",
            "+------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}